{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b9a038f-d3da-4aea-9eca-4e528aebc4c5",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation(RAG) 사용해 보기\n",
    "- 현재 LLM은 `Fine-tuning(특히 Low Rank Adaptation)`과 `Retrieval Augmented Generation(RAG)`라는 방식으로 발전하고 있음\n",
    "- `Fine-tuning`\n",
    "    - 모델의 가중치를 갱신하기 위해 데이터를 추가적으로 학습시키는 방식으로, 상대적으로 오랜 시간이 걸림\n",
    "    - `시험을 치르기 위해 모델에게 공부를 시키는 것`\n",
    "- `RAG`\n",
    "    - LLM은 태생적으로 Hallucination(환각) 증상을 가지고 있음\n",
    "    - 이를 해결하기 위해, LLM이 답변을 위해 필요한 맥락 정보를 같이 전달하여 이를 예방하고 좀 더 원하는 결과를 얻을 수 있음\n",
    "    - `오픈북 시험`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e151ee49-dbf6-4448-a81c-a93452550c36",
   "metadata": {},
   "source": [
    "## 작동 방식\n",
    "1. RAG에 사용할 데이터를 추출\n",
    "2. 데이터를 구조화/청킹하여 메타데이터와 함게 임베딩 및 벡터DB에 저장\n",
    "3. 사용자의 질의를 임베딩\n",
    "4. 벡터DB에서 사용자 질의 임베딩과 유사한 벡터를 찾고 해당 데이터를 추출 이를 문맥 정보로 활용\n",
    "5. LLM에 문맥과 사용자 질의를 전달하여 답변시 좀 더 활용할 수 있도록 함\n",
    "\n",
    "![](img/1.png)\n",
    "![](img/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abab973-0477-4c8f-b47e-e788824f69fc",
   "metadata": {},
   "source": [
    "## PDF 내용을 기준으로 답변해 주는 챗봇 만들어 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c4cf8e-8146-4313-802a-d52c62bde3ab",
   "metadata": {},
   "source": [
    "## 사용할 모델 정의\n",
    "\n",
    "### 임베딩: HuggingFaceEmbeddings\n",
    "\n",
    "### LLM: 42dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b04a814-562c-4fe2-9694-d11ed412494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7ea71ba-0389-45de-9d22-678d3f6180dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_NAME = \"jhgan/ko-sroberta-multitask\"\n",
    "LLM_MODEL_PATH = 'models/42dot_LLM-SFT-1.3B_Q4_K_M.gguf'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dc9c66-96c7-4074-9d34-ad9851331766",
   "metadata": {},
   "source": [
    "## VetorDB 정의\n",
    "- 다양한 DB가 있으나, 예제에서는 `faiss`를 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e53f2056-290f-4eeb-bd50-225412e57ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS  # pip install faiss-cpu 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dcdd6f-5f11-4185-94d4-b7872afd197d",
   "metadata": {},
   "source": [
    "## VectorDB 데이터 생성\n",
    "1. RAG에 사용할 데이터 로드(여기서는 PDF)\n",
    "2. 적절한 형태로 분리\n",
    "3. 각 청크를 임베딩 모델을 이용하여 임베딩 및 VectorDB에 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f0c94-fb38-4f7c-b61b-df75c5feab30",
   "metadata": {},
   "source": [
    "## 데이터 로드\n",
    "- 의미있는 형태로만 가져올 수 있다면 어떤 데이터든 벡터화할 수 있음\n",
    "- 예제에서는 PDF파일(data/개인정보_보호_가이드라인.pdf)의 내용을 가져오는 PDF 파서를 이용함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e06ebe4-4c49-494d-ac3b-e3f5fff2cbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = \"faiss_index_st\"  # 데이터가 저장될/된 곳\n",
    "FILE_PATH = \"data/개인정보_보호_가이드라인.pdf\"\n",
    "\n",
    "# pdf 파서를 이용해 데이터를 읽고 분리\n",
    "# PyPDFLoader는 pypdf라는 라이브러리는 래핑하여 langchain에서 쉽게 사용할 수 있도록 해줌 (pip install pypdf 필요)\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(FILE_PATH)\n",
    "data_list = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ef45b19-67f1-42c8-8a71-4372f7612a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Personal Information Protection Guidelines  • 5\\n  신기술 환경에 부합하는 개인정보 처리기준 제시로 법 실효성 확보\\n•     AI, 영상·생체인식정보·위치정보 처리기기 등 신기술 장치 운영 시 안전한 개인정보 처리·보호 \\n방법·절차 등을 제시\\n•     재택근무, 정보유출 방지 등을 위한 개인정보 처리 시 준수사항 안내\\n  침해사고 예방 및 정보주체 권리 보장\\n•     사업자의 업무 효율과 개인정보 보호에 대한 사회적 관심 증가를 고려하여, 근로자 개인정보 \\n침해사고 예방방안 제시\\n•     개인정보 침해사고 발생 등에 따른 침해신고, 분쟁해결, 손해배상 청구 등 정보주체 권리보장  \\n방법 안내\\n2       적용 대상 및 범위\\n  적용대상\\n•     입사 지원자 및 근로자\\n•     근로자 등의 개인정보를 처리하는 사용자\\n  적용범위 : 채용 준비부터 고용 종료까지 인사·노무 업무 전 과정\\n①  채용 준비 : 채용 전형, 합격자 통보 등과 관련한 개인정보 처리·보호 방법\\n②  채용 결정 : 근로자명부 작성 등 법령 준수, 근로계약의 체결·이행 등에 따른 개인정보 처리·보호\\n③  고용 유지 :   인사이동, 보수·복리후생 등 인력관리에 따른 개인정보 처리·보호 방법 및 디지털 장치 \\n도입 시 준수사항 등\\n④  고용 종료 :   퇴직 근로자의 개인정보 파기 및 경력증명서 발급 등에 필요한 개인정보 처리·보호 방법', metadata={'source': 'data/개인정보_보호_가이드라인.pdf', 'page': 4})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa02d98-bc50-468f-8ced-79f88bee395e",
   "metadata": {},
   "source": [
    "## 데이터 분리\n",
    "- PDF 파서를 통해 가져온 데이터는 한 단락 혹은 페이지 별로 데이터를 가지고 있음\n",
    "- 너무 많은 정보를 하나의 벡터로 표현하는 것은 각 벡터간의 의미가 퇴색되어, 적절하지 않을 수 있음\n",
    "- 이를 위해 여러가지 방법이 존재하며, 본 예제에서는 가장 간단한 형태인 ChracterTextSplitter를 사용하여 특정 크기별로 데이터를 분리함\n",
    "- chunk_overlap을 통해 각 벡터들간의 연관성을 조금씩 두고자 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e9ca5dc-b312-4e8c-b97a-9b11ab8b85fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,  # 300글자 단위로 쪼갬\n",
    "    chunk_overlap=30,  # 30글자를 겹치게 함\n",
    "    length_function=len,  # 청크의 길이 기준이 되는 함수\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "# data_list는 langchain이 사용할 수 있게 래핑된 PyPDFLoader를 사용했기 때문에 langchain 내부에서 사용하는 Document라는 객체 단위로 구분됨\n",
    "# RecursiveCharacterTextSplitter 역시 List[Document]를 받아 List[Document]를 반환함\n",
    "texts = text_splitter.split_documents(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88b0b5e1-e519-488c-86dd-fa95d6f04d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Personal Information Protection Guidelines  • 5\\n  신기술 환경에 부합하는 개인정보 처리기준 제시로 법 실효성 확보\\n•     AI, 영상·생체인식정보·위치정보 처리기기 등 신기술 장치 운영 시 안전한 개인정보 처리·보호 \\n방법·절차 등을 제시\\n•     재택근무, 정보유출 방지 등을 위한 개인정보 처리 시 준수사항 안내\\n  침해사고 예방 및 정보주체 권리 보장\\n•     사업자의 업무 효율과 개인정보 보호에 대한 사회적 관심 증가를 고려하여, 근로자 개인정보 \\n침해사고 예방방안 제시', metadata={'source': 'data/개인정보_보호_가이드라인.pdf', 'page': 4})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67f1cccc-6189-4ec8-b8a6-532496caceb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='침해사고 예방방안 제시\\n•     개인정보 침해사고 발생 등에 따른 침해신고, 분쟁해결, 손해배상 청구 등 정보주체 권리보장  \\n방법 안내\\n2       적용 대상 및 범위\\n  적용대상\\n•     입사 지원자 및 근로자\\n•     근로자 등의 개인정보를 처리하는 사용자\\n  적용범위 : 채용 준비부터 고용 종료까지 인사·노무 업무 전 과정\\n①  채용 준비 : 채용 전형, 합격자 통보 등과 관련한 개인정보 처리·보호 방법\\n②  채용 결정 : 근로자명부 작성 등 법령 준수, 근로계약의 체결·이행 등에 따른 개인정보 처리·보호', metadata={'source': 'data/개인정보_보호_가이드라인.pdf', 'page': 4})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5903f318-304a-44e3-aa11-2391d4a378d5",
   "metadata": {},
   "source": [
    "## 임베딩 모델 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9812f681-8770-45a1-803e-72a853ff19dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sucream/python/ai/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"jhgan/ko-sroberta-multitask\",\n",
    "    cache_folder=\"models\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a17668-421b-4b20-9cb7-46defdb230d6",
   "metadata": {},
   "source": [
    "## 임베딩 작업 및 저장\n",
    "- FAISS, HuggingFaceEmbeddings도 langchain에서 사용하기 좋게 래핑되었기 때문에 쉽게 임베딩 및 db에 저장할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e534fb5e-b933-4983-890a-0914f2c8ec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS VectorDB에 texts 내에 있는 Document를 하나씩 embedding_model을 이용하여 임베딩하고 저장\n",
    "faiss_index = FAISS.from_documents(texts, embedding_model)\n",
    "# 아직까지 VectorDB는 메모리에만 존재함\n",
    "# 이를 위해 save_local 함수로 저장할 수 있음\n",
    "faiss_index.save_local(DB_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9957fca6-13cb-49a5-89b4-d68a43c3fc72",
   "metadata": {},
   "source": [
    "## VectorDB 로드\n",
    "- 로컬에 저장된 DB를 불러올 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b508fdc4-ef6b-4e30-8b36-c30f0a0c935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS를 불러오는데, 향후 이 vectorDB에 임베딩으로 사용할 모델을 같이 넣어줘야 함\n",
    "faiss_index = FAISS.load_local(DB_NAME, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee9a06c-348e-461d-8f4b-ea8fa26240bc",
   "metadata": {},
   "source": [
    "## 질의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b0c24a7-2bf9-4b11-8603-2baf87320dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b38d972-e75b-4c68-956b-560f49ba4ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 vectorDB를 retriever로 사용\n",
    "# k: 3은 사용자 질의와 가장 유사한 데이터 3개를 문맥으로 같이 넘기라는 의미\n",
    "retriever = faiss_index.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ebe1a58-9907-4b73-89d4-6792521aa8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 219 tensors from models/42dot_LLM-SFT-1.3B_Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 24\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5632\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,50304]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,50304]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,50304]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,50000]   = [\"Ġ t\", \"Ġ a\", \"i n\", \"h e\", \"r e\",...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 50256\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 50256\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 50256\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 50258\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   49 tensors\n",
      "llama_model_loader: - type q4_K:  145 tensors\n",
      "llama_model_loader: - type q6_K:   25 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 48/50304 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 50304\n",
      "llm_load_print_meta: n_merges         = 50000\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 24\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 5632\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 1.44 B\n",
      "llm_load_print_meta: model size       = 844.15 MiB (4.92 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 50256 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 50256 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 50256 '<|endoftext|>'\n",
      "llm_load_print_meta: PAD token        = 50258 '<||pad||>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_tensors: ggml ctx size       =    0.08 MiB\n",
      "llm_load_tensors: system memory used  =  844.23 MiB\n",
      "......................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 32768\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  = 6144.00 MiB, K (f16): 3072.00 MiB, V (f16): 3072.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 508/508\n",
      "llama_new_context_with_model: compute buffer total size = 36.38 MiB\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "llm = LlamaCpp(\n",
    "    model_path=LLM_MODEL_PATH,\n",
    "    temperature=0,\n",
    "    top_p=0.98,\n",
    "    top_k=0,\n",
    "    n_ctx=32768,\n",
    "    max_tokens=1024,\n",
    "    repeat_penalty=1.176,\n",
    "    last_n_tokens_size=128,\n",
    "    verbose=False,\n",
    "    stop=[\"</s>\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc8bef89-33f0-445c-b0b1-9dcfeb9a5284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM에 전달할 프롬프트 생성\n",
    "# LLM이 문맥을 이용할 수 있도록 context를 하나 더 만듦\n",
    "\n",
    "template = \"\"\"\\\n",
    "아래는 대답을 위한 문맥 정보입니다.\n",
    "---------------------\n",
    "{context}\n",
    "---------------------\n",
    "주어진 문맥 정보와 사전 지식을 활용하여, 다음 질문에 대한 답변을 해주세요.\n",
    "만약 답변을 찾을 수 없다면, '죄송합니다. 잘 모르겠어요.'라고 답변하세요.\n",
    "\n",
    "질문: {question}\n",
    "답변: \\\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "194dab21-e246-4c7d-81be-4981882b10df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 사용할 최종 모델\n",
    "retrievalQA = RetrievalQA.from_llm(llm, prompt, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6de03f7f-bee0-4092-aa66-7a52a6b2f324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문:  근로자의 가족 개인정보를 확인한 후 적절한 복지혜택을 제공하고자 합니다. 이 경우에 근로자 가족의 동의를 받아야 하나요?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 가족수당의 기초가 되거나 가족 복지혜택 제공을 위해 필요한 개인정보의 수집은 근로자 또는 가족의 동의 없이도 가능합니다. 다만 고유식별정보나 민감정보를 수집하는 경우에는 별도의 동의를 받도록 해야 하며 주민등록번호는 관련 법령에 따라 처리해야 합니다.\n"
     ]
    }
   ],
   "source": [
    "query = input(\"질문: \")\n",
    "print(retrievalQA.invoke(query)[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
